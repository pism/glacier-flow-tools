{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853cb685-f74f-415f-9061-9c9a1f74898e",
   "metadata": {},
   "source": [
    "# Extract profiles (Aschwanden, Truffer, and Fahnestock, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a31451-dff4-4ea7-bdd9-a64192983132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "\n",
    "import geopandas as gp\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pylab as plt\n",
    "from matplotlib import colors, cm\n",
    "from matplotlib.colors import LightSource\n",
    "import matplotlib.ticker as mticker\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "from pypism import profiles\n",
    "from pypism.profiles import process_profile\n",
    "from pypism.utils import preprocess_nc\n",
    "from pypism.hillshade import hillshade\n",
    "from pypism.utils import qgis2cmap, tqdm_joblib, blend_multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0eda5-7299-4f22-baa0-66cca023319b",
   "metadata": {},
   "source": [
    "## Resolution along profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e933df45-9218-4c8c-8b38-06603ded176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_resolution = 200 # m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b62990-1ce1-47fc-a61a-74f518721ca2",
   "metadata": {},
   "source": [
    "## Load profiles and segmentize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4293ffc2-920d-4853-8a07-6376071d83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_path = Path(\"../data/greenland-flux-gates.gpkg\")\n",
    "profiles_gp = gp.read_file(profiles_path).rename(columns={\"id\": \"profile_id\"})\n",
    "geom = profiles_gp.segmentize(profile_resolution)\n",
    "profiles_gp = gp.GeoDataFrame(profiles_gp, geometry=geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001bda2b-f2d7-4dac-8adf-1c5f366ac9b5",
   "metadata": {},
   "source": [
    "## Load observed velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3062049-fd94-4348-b1b1-e415450fc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_file = Path(\"/Users/andy/Google Drive/My Drive/data/ITS_LIVE/GRE_G0240_0000.nc\")\n",
    "obs_ds = xr.open_dataset(obs_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841f69f-a65b-4490-94c6-cdcc19e95862",
   "metadata": {},
   "source": [
    "## Load Ensemble Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df9fed-6c19-4cc7-a0bb-a4585632a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pism_files = list(Path(\"/Users/andy/Google Drive/My Drive/Projects/gris-calib/data\").glob(\"velsurf_mag_gris*.nc\"))\n",
    "sim_ds = xr.open_mfdataset(pism_files, \n",
    "                  preprocess=preprocess_nc,\n",
    "                  concat_dim=\"exp_id\",\n",
    "                  combine=\"nested\",\n",
    "                  parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7df8aa-a331-4835-9964-4e520ce5650a",
   "metadata": {},
   "source": [
    "## Extract all profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f4b43-994e-4732-8242-dc29a47928c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.apply_ufunc?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb42e6e-f164-4ef5-a417-53e48df3c209",
   "metadata": {},
   "source": [
    "## Plot profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3035ec9-8793-4ba4-a173-a0fd530dfa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run ../profiles/compute_profiles.py --thickness_url \"~/Google Drive/My Drive/data/MCdataset/BedMachineGreenland-v5.nc\" --n_jobs 4 --alpha 0.05 --result_dir ../profiles/2024_04_test --velocity_url GRE_G0240_0000.nc --profiles_url ../data/greenland-flux-gates.gpkg \"~/Google Drive/My Drive/Projects/gris-calib/2022_09_*/state/gris_g1800m_v4_id_0_0_50.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6b0cf-3235-4f64-be99-dfc99044e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0d018-66bf-465b-a2c7-432e0a329449",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_x_da.name = \"mass_flux_x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df50c3-0d80-4666-9155-16d85526c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from matplotlib import cm, colors\n",
    "from matplotlib.colors import LightSource\n",
    "\n",
    "from glacier_flow_tools.utils import blend_multiply, figure_extent, get_dataarray_extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80d0f3-941c-41a4-aa62-961ff2dba5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os = obs_sims_profiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688f2b1-f0c7-4936-b6d4-5cb00aa407b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_x_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a4328-0ff5-4512-9f82-8df9e854aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from matplotlib import cm, colors\n",
    "from matplotlib.colors import LightSource\n",
    "from glacier_flow_tools.utils import blend_multiply, figure_extent, get_dataarray_extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5588c-2c8a-4338-a38f-072c632724cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_glacier(\n",
    "    profile_series: gp.GeoSeries,\n",
    "    surface: xr.DataArray,\n",
    "    overlay: xr.DataArray,\n",
    "    result_dir: Union[str, Path],\n",
    "    cmap=\"viridis\",\n",
    "    vmin: float = 10,\n",
    "    vmax: float = 1500,\n",
    "    ticks: Union[List[float], np.ndarray] = [10, 100, 250, 500, 750, 1500],\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a surface over a hillshade, add profile and correlation coefficient.\n",
    "\n",
    "    This function plots a surface over a hillshade, adds a profile and correlation coefficient.\n",
    "    The plot is saved as a PDF file in the specified result directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profile_series : gp.GeoSeries\n",
    "        The profile to be plotted.\n",
    "    surface : xr.DataArray\n",
    "        The surface to be plotted over the hillshade.\n",
    "    overlay : xr.DataArray\n",
    "        The overlay to be added to the plot.\n",
    "    result_dir : Union[str, Path]\n",
    "        The directory where the result PDF file will be saved.\n",
    "    cmap : str, optional\n",
    "        The colormap to be used for the plot, by default \"viridis\".\n",
    "    vmin : float, optional\n",
    "        The minimum value for the colormap, by default 10.\n",
    "    vmax : float, optional\n",
    "        The maximum value for the colormap, by default 1500.\n",
    "    ticks : Union[List[float], np.ndarray], optional\n",
    "        The ticks to be used for the colorbar, by default [10, 100, 250, 500, 750, 1500].\n",
    "    \"\"\"\n",
    "\n",
    "    geom = getattr(profile_series, \"geometry\")\n",
    "    geom_centroid = geom.centroid\n",
    "    profile_centroid = gp.GeoDataFrame([profile_series], geometry=[geom_centroid])\n",
    "    profile = gp.GeoDataFrame([profile_series], geometry=[geom])\n",
    "    glacier_name = getattr(profile, \"profile_name\").values[0]\n",
    "    exp_id = getattr(profile, \"exp_id\").values[0]\n",
    "    x_c = round(profile_centroid.geometry.x.values[0])\n",
    "    y_c = round(profile_centroid.geometry.y.values[0])\n",
    "    extent_slice = figure_extent(x_c, y_c)\n",
    "    cartopy_crs = ccrs.NorthPolarStereo(central_longitude=-45, true_scale_latitude=70, globe=None)\n",
    "    # Shade from the northwest, with the sun 45 degrees from horizontal\n",
    "    light_source = LightSource(azdeg=315, altdeg=45)\n",
    "    glacier_overlay = overlay.sel(extent_slice)\n",
    "    glacier_surface = surface.interp_like(glacier_overlay)\n",
    "\n",
    "    extent = get_dataarray_extent(glacier_overlay)\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    mapper = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "    v = mapper.to_rgba(glacier_overlay.to_numpy())\n",
    "    z = glacier_surface.to_numpy()\n",
    "    fig = plt.figure(figsize=(6.2, 6.2))\n",
    "    ax = fig.add_subplot(111, projection=cartopy_crs)\n",
    "    rgb = light_source.shade_rgb(v, elevation=z, vert_exag=0.01, blend_mode=blend_multiply)\n",
    "    # Use a proxy artist for the colorbar...\n",
    "    im = ax.imshow(v, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    im.remove()\n",
    "    corr = ax.imshow(\n",
    "        v,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cmap=\"RdYlGn\",\n",
    "    )\n",
    "    corr.remove()\n",
    "    ax.imshow(rgb, extent=extent, origin=\"upper\", transform=cartopy_crs)\n",
    "    profile.plot(ax=ax, color=\"k\", lw=1)\n",
    "    profile_centroid.plot(\n",
    "        column=\"pearson_r\", vmin=0, vmax=1, cmap=\"RdYlGn\", markersize=50, legend=False, missing_kwds={}, ax=ax\n",
    "    )\n",
    "    ax.annotate(f\"{glacier_name}\", (x_c, y_c), (10, 10), xycoords=\"data\", textcoords=\"offset points\")\n",
    "    ax.gridlines(\n",
    "        draw_labels={\"top\": \"x\", \"left\": \"y\"},\n",
    "        dms=True,\n",
    "        xlocs=np.arange(-50, 0, 1),\n",
    "        ylocs=np.arange(50, 88, 1),\n",
    "        x_inline=False,\n",
    "        y_inline=False,\n",
    "        rotate_labels=20,\n",
    "        ls=\"dotted\",\n",
    "        color=\"k\",\n",
    "    )\n",
    "\n",
    "    ax.set_extent(extent, crs=cartopy_crs)\n",
    "    fig.colorbar(im, ax=ax, shrink=0.5, pad=0.025, label=overlay.units, extend=\"max\", ticks=ticks)\n",
    "    fig.colorbar(\n",
    "        corr, ax=ax, shrink=0.5, pad=0.025, label=\"Pearson $r$ (1)\", orientation=\"horizontal\", location=\"bottom\"\n",
    "    )\n",
    "    fig.savefig(result_dir / Path(f\"{glacier_name}_{exp_id}_speed.pdf\"))\n",
    "    plt.close()\n",
    "    del fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758cb11-dff5-4dca-a175-fb1eab4f1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4807ea4-4adc-4c9f-b6eb-99583a2a99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dede6e-556a-4cad-8214-1d5cc3ff52b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_dict = {'profile_id': {0: 0, 1: 1},\n",
    " 'profile_name': {0: 'Horizontal Gletscher', 1: 'Vertical'},\n",
    " 'geometry': {0: LineString([[-10, 0], [10, 0]]),\n",
    "  1: LineString([[0, -10],[0, 10]])}}\n",
    "profiles_gp = gp.GeoDataFrame.from_dict(profiles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a27b0c1-c7a7-434d-940b-63261f843d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import get_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a1b33-7766-47b4-9069-9bc623143194",
   "metadata": {},
   "outputs": [],
   "source": [
    "        nx = 201\n",
    "        ny = 201\n",
    "        x_min = -10\n",
    "        x_max = 10\n",
    "        y_min = -10\n",
    "        y_max = 10\n",
    "        x = np.linspace(x_min, x_max, nx)\n",
    "        y = np.linspace(y_min, y_max, ny)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "        rng = np.random.default_rng(seed=42)\n",
    "        # Directional vectors\n",
    "        vx = np.zeros_like(X)\n",
    "        vy = (y_max - Y**2) + rng.random(size=Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb431e-6f32-4f82-b52c-0dad0b010bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng.random(size=Y.size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f090a8-bc7c-4dc1-97fe-69f0c37cf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import get_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c6834-fb01-4ce9-9239-a52d050b137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21172136-212e-4d74-9528-0a8f67066406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    nx = 201\n",
    "    ny = 201\n",
    "    x_min = -10\n",
    "    x_max = 10\n",
    "    y_min = -10\n",
    "    y_max = 10\n",
    "    x = np.linspace(x_min, x_max, nx)\n",
    "    y = np.linspace(y_min, y_max, ny)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    # Directional vectors\n",
    "    vx = (y_max**2 - Y**2) + rng.random(size=Y.shape)\n",
    "    vy = np.zeros_like(X)\n",
    "\n",
    "    coords = {\n",
    "        \"x\": (\n",
    "            [\"x\"],\n",
    "            x,\n",
    "            {\n",
    "                \"units\": \"m\",\n",
    "                \"axis\": \"X\",\n",
    "                \"standard_name\": \"projection_x_coordinate\",\n",
    "                \"long_name\": \"x-coordinate in projected coordinate system\",\n",
    "            },\n",
    "        ),\n",
    "        \"y\": (\n",
    "            [\"y\"],\n",
    "            y,\n",
    "            {\n",
    "                \"units\": \"m\",\n",
    "                \"axis\": \"Y\",\n",
    "                \"standard_name\": \"projection_y_coordinate\",\n",
    "                \"long_name\": \"y-coordinate in projected coordinate system\",\n",
    "            },\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"vx\": xr.DataArray(\n",
    "                data=vx,\n",
    "                dims=[\"y\", \"x\"],\n",
    "                coords=coords,\n",
    "                attrs={\"standard_name\": \"velocity in x-direction\", \"units\": \"m/yr\"},\n",
    "            ),\n",
    "            \"vy\": xr.DataArray(\n",
    "                data=vy,\n",
    "                dims=[\"y\", \"x\"],\n",
    "                coords=coords,\n",
    "                attrs={\"standard_name\": \"velocity in y-direction\", \"units\": \"m/yr\"},\n",
    "            ),\n",
    "        },\n",
    "        attrs={\"Conventions\": \"CF-1.7\"},\n",
    "    )\n",
    "    ds[\"Polar_Stereographic\"] = int()\n",
    "    ds.Polar_Stereographic.attrs[\"grid_mapping_name\"] = \"polar_stereographic\"\n",
    "    ds.Polar_Stereographic.attrs[\"false_easting\"] = 0.0\n",
    "    ds.Polar_Stereographic.attrs[\"false_northing\"] = 0.0\n",
    "    ds.Polar_Stereographic.attrs[\"latitude_of_projection_origin\"] = 90.0\n",
    "    ds.Polar_Stereographic.attrs[\"scale_factor_at_projection_origin\"] = 1.0\n",
    "    ds.Polar_Stereographic.attrs[\"standard_parallel\"] = 70.0\n",
    "    ds.Polar_Stereographic.attrs[\"straight_vertical_longitude_from_pole\"] = -45\n",
    "    ds.Polar_Stereographic.attrs[\"proj_params\"] = \"epsg:3413\"\n",
    "    quadratic_flow = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79345f-a245-4a5f-b405-725f28373e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ec2b1-9084-41a7-8e9d-15cb57192de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from glacier_flow_tools import profiles\n",
    "    profiles_dict = {\n",
    "        \"profile_id\": {0: 0, 1: 1},\n",
    "        \"profile_name\": {0: \"Horizontal Gletscher\", 1: \"Vertical\"},\n",
    "        \"geometry\": {0: LineString([[-10, 0], [10, 0]]), 1: LineString([[0, -10], [0, 10]])},\n",
    "    }\n",
    "    profiles_gp = gp.GeoDataFrame.from_dict(profiles_dict)\n",
    "    geom = profiles_gp.segmentize(1.0)\n",
    "    profiles_gp = gp.GeoDataFrame(profiles_gp, geometry=geom)\n",
    "    profiles_gp = profiles_gp[[\"profile_id\", \"profile_name\", \"geometry\"]]\n",
    "\n",
    "    profile = profiles_gp.loc[[0]]\n",
    "    geom = getattr(profile, \"geometry\")\n",
    "    x_p, y_p = get_coordinates(geom).T\n",
    "    profile_name = getattr(profile, \"profile_name\").values[0]\n",
    "    profile_id = getattr(profile, \"profile_id\").values[0]\n",
    "\n",
    "    kwargs: Dict = {}\n",
    "    x_profile = quadratic_flow.profiles.extract_profile(x_p, y_p, profile_name=profile_name, profile_id=profile_id, **kwargs)\n",
    "\n",
    "    profile = profiles_gp.loc[[1]]\n",
    "    geom = getattr(profile, \"geometry\")\n",
    "    x_p, y_p = get_coordinates(geom).T\n",
    "    profile_name = getattr(profile, \"profile_name\").values[0]\n",
    "    profile_id = getattr(profile, \"profile_id\").values[0]\n",
    "    x_vy_true = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0.])\n",
    "    assert_array_almost_equal(x_profile.vy, x_vy_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de00c7-bb2f-475e-956f-592ae67c5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_profile.vy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28eaac-8cfc-47f5-b0ae-adfd1d266b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs.update({\"compute_profile_normal\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac053b2c-d474-4dd1-b987-e3dea4865920",
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_profile = quadratic_flow.profiles.extract_profile(x_p, y_p, profile_name=profile_name, profile_id=profile_id, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e24ed9-0f5e-4fc9-a84b-f392b474b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    profile = profiles_gp.loc[[1]]\n",
    "    geom = getattr(profile, \"geometry\")\n",
    "    x_p, y_p = get_coordinates(geom).T\n",
    "    profile_name = getattr(profile, \"profile_name\").values[0]\n",
    "    profile_id = getattr(profile, \"profile_id\").values[0]\n",
    "    x_vy_true = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0.])\n",
    "    assert_array_almost_equal(x_profile.vy, x_vx_true)\n",
    "\n",
    "    kwargs: Dict = {}\n",
    "    y_profile = quadratic_flow.profiles.extract_profile(x_p, y_p, profile_name=profile_name, profile_id=profile_id, **kwargs)\n",
    "array([  0.90858069,  19.51916723,  36.95104485,  51.28749629,\n",
    "        64.28259608,  75.47610046,  84.02253044,  91.83760948,\n",
    "        96.92277666,  99.93217176, 100.29915991,  99.22948628,\n",
    "        96.05096098,  91.20227129,  84.15173917,  75.92502223,\n",
    "        64.51036844,  51.41748735,  36.1520003 ,  19.53233673,\n",
    "         0.23981163])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb2f83-da01-4af6-89f8-e2520d3691fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_profile = quadratic_flow.profiles.extract_profile(x_p, y_p, profile_name=profile_name, profile_id=profile_id, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108162f6-2cf9-49fc-b20f-240db292552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_var = \"v_normal\"\n",
    "normal_component_vars: dict = {\"x\": \"vx\", \"y\": \"vy\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a556d-601a-4e38-a65b-fdc1436cf252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68883513-536d-4c5a-b09c-2cd4a0d1061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_profile.vx.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a2e0b-8ef8-44a5-905e-b3d343d6d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    profile = profiles_gp.loc[[0]]\n",
    "    geom = getattr(profile, \"geometry\")\n",
    "    x_p, y_p = get_coordinates(geom).T\n",
    "    profile_name = getattr(profile, \"profile_name\").values[0]\n",
    "    profile_id = getattr(profile, \"profile_id\").values[0]\n",
    "\n",
    "    kwargs: Dict = {}\n",
    "    x_profile = quadratic_flow.profiles.extract_profile(x_p, y_p, profile_name=profile_name, profile_id=profile_id, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9068563-0b84-4ce9-9061-2d6cfee578fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    nx = 201\n",
    "    ny = 201\n",
    "    x_min = -10\n",
    "    x_max = 10\n",
    "    y_min = -10\n",
    "    y_max = 10\n",
    "    x = np.linspace(x_min, x_max, nx)\n",
    "    y = np.linspace(y_min, y_max, ny)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    # Directional vectors\n",
    "    vy = np.zeros_like(Y)\n",
    "    vx = (y_max**2 - Y**2) + rng.random(size=Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8f8f9-ffa0-4cc4-918b-3de58953d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vx)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bdc7e8-d7bc-4206-ae5b-945bbde1f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaddea3-b476-49e2-a7d6-1fd600e3996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(-(-Y**2))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf5c78-22e8-47d1-bf90-e0399ab0a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, y_max -y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26660e-a010-4fd5-8b25-621d1be0c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470afe5-d5ee-4abd-85f7-64e2b2669f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_xarray.units.units.kelvin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8066ff-eb48-4d8a-a155-5098200606b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import cf_xarray.units\n",
    "import pint_xarray\n",
    "ds = xr.open_dataset(\"../pism_test_pint_xarray.nc\")\n",
    "for v in ds.data_vars:\n",
    "    da = ds[v]\n",
    "    da.pint.quantify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1954d-660d-4559-9a2f-7a3872682a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(\"../pism_test_pint_xarray*.nc\").drop_vars([\"shelfbtemp\", \"effective_ice_surface_temp\", \"ice_surface_temp\", \"hardav\"]).chunk(\"auto\").pint.quantify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c06b8-53f9-46b9-838a-4fb649dcf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_flow_tools.utils import preprocess_nc\n",
    "from functools import partial\n",
    "ds = xr.open_mfdataset(\"../gris_g1800m_v4_id_*_0_50.nc\",\n",
    "                       preprocess=partial(preprocess_nc, drop_dims=[\"z\", \"zb\"], drop_vars=[\"timestamp\", \"shelfbtemp\", \"effective_ice_surface_temp\", \"ice_surface_temp\", \"hardav\"]),\n",
    "        concat_dim=\"exp_id\",\n",
    "        combine=\"nested\",\n",
    "        chunks=\"auto\",\n",
    "        engine=\"h5netcdf\",\n",
    "        parallel=True,\n",
    "        decode_times=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b6b4b-70e2-4fca-8fef-af46ca27166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = xr.open_dataset(\"../gris_g1800m_v4_id_bar_0_50.nc\").chunk(\"auto\")\n",
    "ds2 = xr.open_dataset(\"../gris_g1800m_v4_id_foo_0_50.nc\").chunk(\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = xr.open_dataset(\"~/Google Drive/My Drive/Projects/gris-calib/2022_09_thermo/state/gris_g1800m_v4_id_0_0_50.nc\").chunk(\"auto\")\n",
    "ds2 = xr.open_dataset(\"~/Google Drive/My Drive/Projects/gris-calib/2022_09_thermo/state/gris_g1800m_v4_id_0_0_50.nc\").chunk(\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a69b0-2b65-4aab-a393-7d06026985d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_p = preprocess_nc(ds1, drop_dims=[\"z\", \"zb\"], drop_vars=[\"timestamp\", \"shelfbtemp\", \"effective_ice_surface_temp\", \"ice_surface_temp\", \"hardav\"])\n",
    "ds2_p = preprocess_nc(ds2, drop_dims=[\"z\", \"zb\"], drop_vars=[\"timestamp\", \"shelfbtemp\", \"effective_ice_surface_temp\", \"ice_surface_temp\", \"hardav\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8e12f-1643-4d5e-99a0-3a13452bfc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged  = xr.concat([ds1_p, ds2_p], dim=\"exp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec168a-346d-4157-ad00-eaf809e61bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.pint.quantify().thk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1153d72-c69d-40cd-9ab2-2a8800318ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(\"~/Google Drive/My Drive/Projects/gris-calib/2022_09_*/state/gris_g1800m_v4_id_0_0_50.nc\",\n",
    "                       preprocess=partial(preprocess_nc, drop_dims=[\"z\", \"zb\"], drop_vars=[\"timestamp\", \"shelfbtemp\", \"effective_ice_surface_temp\", \"ice_surface_temp\", \"hardav\"]),\n",
    "        concat_dim=\"exp_id\",\n",
    "        combine=\"nested\",\n",
    "        chunks=\"auto\",\n",
    "        engine=\"h5netcdf\",\n",
    "        parallel=True,\n",
    "        decode_times=False,\n",
    "    )\n",
    "\n",
    "def preprocess_nc(\n",
    "    ds: xr.Dataset,\n",
    "    regexp: str = \"id_(.+?)_\",\n",
    "    dim: str = \"exp_id\",\n",
    "    drop_vars: Union[str, Iterable[Hashable], Callable[[xr.Dataset], Union[str, Iterable[Hashable]]]] = [\"nv4\"],\n",
    "    drop_dims: List[str] = [\"nv4\"],\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Add experiment 'exp_id' to the dataset and drop specified variables and dimensions.\n",
    "\n",
    "    This function adds an experiment id ('exp_id') to the dataset, extracted from the source encoding\n",
    "    using the provided regular expression. It then drops the specified variables and dimensions from the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "        The dataset to be preprocessed.\n",
    "    regexp : str, optional\n",
    "        The regular expression used to extract the experiment id from the source encoding, by default \"id_(.+?)_\".\n",
    "    dim : str, optional\n",
    "        The name of the dimension to be added to the dataset, by default \"exp_id\".\n",
    "    drop_vars : Union[List[str], None], optional\n",
    "        The variables to be dropped from the dataset, by default None.\n",
    "    drop_dims : List[str], optional\n",
    "        The dimensions to be dropped from the dataset, by default [\"nv4\"].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        The preprocessed dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    m_id_re = re.search(regexp, ds.encoding[\"source\"])\n",
    "    ds.expand_dims(dim)\n",
    "    assert m_id_re is not None\n",
    "    m_id: Union[str, int]\n",
    "    try:\n",
    "        m_id = int(m_id_re.group(1))\n",
    "    except:\n",
    "        m_id = str(m_id_re.group(1))\n",
    "    ds[dim] = m_id\n",
    "    return ds.drop_vars(drop_vars, errors=\"ignore\").drop_dims(drop_dims, errors=\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32ba33-269a-475e-ad3a-d6a7afbb54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.pint.quantify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944aa3c6-96fe-49d1-8ae5-baea2085afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.pint.dequantify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcc702-91ab-41a2-908f-1eccae7d4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.pint.dequantify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2bc4d-a89e-42c4-996c-e87c98f8fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.thk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663daed-a712-4e4d-84aa-82e5d5fcc8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.thk.pint.quantify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8daca27-6f68-4d96-80d1-8a3b059d3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430cabe-6680-4973-b8b1-b9f627dbb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg = pint.UnitRegistry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0413fd-484d-477b-a195-5f305284e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg.parse_units(\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3dd67c-b461-4c9a-ad7d-99ed53a709d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg.parse_units_as_container(ds.thk.attrs[\"units\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c6211-c8ad-42bd-a899-2b758d0b00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a3c60-72a1-47af-a2f9-9c3a90e9fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.thk.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a26b7-5247-493d-839a-72b770240c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.thk.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49daa8c5-5c5c-44b1-844b-0cd3e7db38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ds = xr.open_dataset(\"../bar.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8a30e-46ad-43ed-9651-8ff701b097c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ds.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e77486-d75f-4a76-81f6-7c4cb8fbdf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cf_xarray.units\n",
    "import pint_xarray\n",
    "\n",
    "import cftime\n",
    "import xarray as xr\n",
    "import re\n",
    "\n",
    "def preprocess_nc(\n",
    "    ds: xr.Dataset,\n",
    "    regexp: str = \"id_(.+?)_\",\n",
    "    dim: str = \"exp_id\",\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Add experiment 'exp_id' to the dataset.\n",
    "\n",
    "    This function adds an experiment id ('exp_id') to the dataset, extracted from the source encoding\n",
    "    using the provided regular expression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "        The dataset to be preprocessed.\n",
    "    regexp : str, optional\n",
    "        The regular expression used to extract the experiment id from the source encoding, by default \"id_(.+?)_\".\n",
    "    dim : str, optional\n",
    "        The name of the dimension to be added to the dataset, by default \"exp_id\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        The preprocessed dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    m_id_re = re.search(regexp, ds.encoding[\"source\"])\n",
    "    ds.expand_dims(dim)\n",
    "    assert m_id_re is not None\n",
    "    m_id: Union[str, int]\n",
    "    try:\n",
    "        m_id = int(m_id_re.group(1))\n",
    "    except:\n",
    "        m_id = str(m_id_re.group(1))\n",
    "    ds[dim] = m_id\n",
    "    return ds\n",
    "\n",
    "\n",
    "d = {'coords': {'lat': {'dims': ('y', 'x'),\n",
    "   'attrs': {'units': 'degree_north',\n",
    "    'valid_range': [-90.0, 90.0],\n",
    "    'long_name': 'latitude',\n",
    "    'standard_name': 'latitude'},\n",
    "   'data': [[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0]]},\n",
    "  'lon': {'dims': ('y', 'x'),\n",
    "   'attrs': {'units': 'degree_east',\n",
    "    'valid_range': [-180.0, 180.0],\n",
    "    'long_name': 'longitude',\n",
    "    'standard_name': 'longitude'},\n",
    "   'data': [[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0]]},\n",
    "  'time': {'dims': ('time',),\n",
    "   'attrs': {'axis': 'T', 'long_name': 'time'},\n",
    "   'data': [cftime.DatetimeNoLeap(1001, 1, 1, 0, 0, 0, 0, has_year_zero=True)]},\n",
    "  'x': {'dims': ('x',),\n",
    "   'attrs': {'units': 'm',\n",
    "    'axis': 'X',\n",
    "    'long_name': 'X-coordinate in Cartesian system',\n",
    "    'standard_name': 'projection_x_coordinate',\n",
    "    'spacing_meters': 375000.0},\n",
    "   'data': [-750000.0, -375000.0, 0.0, 375000.0, 750000.0]},\n",
    "  'y': {'dims': ('y',),\n",
    "   'attrs': {'units': 'm',\n",
    "    'axis': 'Y',\n",
    "    'long_name': 'Y-coordinate in Cartesian system',\n",
    "    'standard_name': 'projection_y_coordinate',\n",
    "    'spacing_meters': 375000.0},\n",
    "   'data': [-750000.0, -375000.0, 0.0, 375000.0, 750000.0]}},\n",
    " 'attrs': {'Conventions': 'CF-1.6'},\n",
    " 'dims': {'y': 5, 'x': 5, 'time': 1},\n",
    " 'data_vars': {'thk': {'dims': ('time', 'y', 'x'),\n",
    "   'attrs': {'units': 'm',\n",
    "    'valid_min': 0.0,\n",
    "    'long_name': 'land ice thickness',\n",
    "    'standard_name': 'land_ice_thickness'},\n",
    "   'data': [[[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "     [0.0, 0.0, 499.6684394216143, 0.0, 0.0],\n",
    "     [0.0, 499.6684394216143, 499.66844085642595, 499.6684394216143, 0.0],\n",
    "     [0.0, 0.0, 499.6684394216143, 0.0, 0.0],\n",
    "     [0.0, 0.0, 0.0, 0.0, 0.0]]]}}}\n",
    "\n",
    "ds = xr.Dataset.from_dict(d)\n",
    "\n",
    "file1 = \"test_id_foo_0_1000.nc\"\n",
    "file2 = \"test_id_bar_0_1000.nc\"\n",
    "\n",
    "ds.to_netcdf(file1)\n",
    "ds.to_netcdf(file2)\n",
    "\n",
    "ds1 = preprocess_nc(xr.open_dataset(file1)).chunk(\"auto\")\n",
    "ds2 = preprocess_nc(xr.open_dataset(file2)).chunk(\"auto\")\n",
    "single_ds = xr.concat([ds1, ds2], dim=\"exp_id\")\n",
    "single_ds.pint.quantify()\n",
    "\n",
    "mf_ds = xr.open_mfdataset(\"test_id_*.nc\",\n",
    "                          preprocess=preprocess_nc,\n",
    "                          concat_dim=\"exp_id\",\n",
    "                          combine=\"nested\",\n",
    "                          chunks=\"auto\",\n",
    "                          parallel=False,\n",
    "                          decode_times=False)\n",
    "\n",
    "mf_ds.pint.quantify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90120b1-2c3c-48f6-bb8f-2c23a838605a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95a592-a598-4278-ac86-88cdd316270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"xarray.__version__ = \", xr.__version__)\n",
    "print(\"pint_xarray.__version__ = \", pint_xarray.__version__)\n",
    "print(\"cf_xarray.__version__ = \", cf_xarray.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a889c-697c-4957-a01d-a049ac33f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_xarray.units.units.Celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426ad7a-926e-42e3-9f53-f7a3d11b3a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
